{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nparams(model):\n",
    "    \"\"\"Calculate the total number of model parameters\"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-1_8b-chat\"\n",
    "# åŠ è½½tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nparams(model)  # 1889110016 => 1.9B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-20b-chat\"\n",
    "# åŠ è½½tokenizer\n",
    "tokenizer_20b = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model_20b = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_20b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nparams(model_20b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§‘â€ğŸ’»Chat with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=2048):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # InternLM2 çš„è¾“å‡ºå¯èƒ½åŒ…å«æ•´ä¸ªå¯¹è¯å†å²ï¼Œæˆ‘ä»¬åªéœ€è¦æœ€åçš„å›å¤\n",
    "    return response.split(\"Human:\")[-1].split(\"Assistant:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å¼€å§‹èŠå¤©! (è¾“å…¥ 'quit' ç»“æŸå¯¹è¯)\")\n",
    "\n",
    "chat_history = \"<s>Human: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\\nAssistant: æ‚¨å¥½ï¼æˆ‘æ˜¯ InternLM2ï¼Œä¸€ä¸ªç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘è¢«è®¾è®¡ç”¨æ¥ååŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘æ‹¥æœ‰å¹¿æ³›çš„çŸ¥è¯†åº“ï¼Œå¯ä»¥è®¨è®ºå¤šç§ä¸»é¢˜ï¼Œä½†è¯·è®°ä½ï¼Œæˆ‘çš„çŸ¥è¯†å¯èƒ½æœ‰ä¸€å®šå±€é™æ€§ï¼Œå¹¶ä¸”å¯èƒ½ä¸æ€»æ˜¯å®Œå…¨å‡†ç¡®ã€‚æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œå¸®åŠ©ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºæ‚¨åšçš„å—ï¼Ÿ\\nHuman: æ˜ç™½äº†ï¼Œè°¢è°¢ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹èŠå¤©å§ã€‚\\nAssistant: éå¸¸å¥½ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½ä¸æ‚¨èŠå¤©ã€‚æ‚¨æœ‰ä»€ä¹ˆç‰¹åˆ«æƒ³è®¨è®ºçš„è¯é¢˜å—ï¼Ÿæˆ–è€…æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜æƒ³é—®æˆ‘ï¼Ÿæ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»ã€å·¥ä½œå­¦ä¹ ï¼Œè¿˜æ˜¯ç§‘æŠ€ã€æ–‡åŒ–ã€å†å²ç­‰æ–¹é¢çš„è¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸æ‚¨äº¤æµã€‚è¯·éšæ„å¼€å§‹æˆ‘ä»¬çš„å¯¹è¯å§ï¼\\n\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Human: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    chat_history += f\"Human: {user_input}\\nAssistant: \"\n",
    "    response = generate_response(chat_history)\n",
    "    chat_history += f\"{response}\\nHuman: \"\n",
    "    \n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "print(\"å¯¹è¯ç»“æŸ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
