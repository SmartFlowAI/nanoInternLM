{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nparams(model):\n",
    "    \"\"\"Calculate the total number of model parameters\"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-1_8b-chat\"\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nparams(model)  # 1889110016 => 1.9B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-20b-chat\"\n",
    "# 加载tokenizer\n",
    "tokenizer_20b = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model_20b = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_20b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nparams(model_20b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧑‍💻Chat with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=2048):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # InternLM2 的输出可能包含整个对话历史，我们只需要最后的回复\n",
    "    return response.split(\"Human:\")[-1].split(\"Assistant:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"开始聊天! (输入 'quit' 结束对话)\")\n",
    "\n",
    "chat_history = \"<s>Human: 你好，请介绍一下你自己。\\nAssistant: 您好！我是 InternLM2，一个由上海人工智能实验室开发的大型语言模型。我被设计用来协助用户完成各种任务，包括回答问题、提供信息、进行对话等。我拥有广泛的知识库，可以讨论多种主题，但请记住，我的知识可能有一定局限性，并且可能不总是完全准确。我会尽力为您提供有用的信息和帮助。有什么我可以为您做的吗？\\nHuman: 明白了，谢谢。接下来我们开始聊天吧。\\nAssistant: 非常好，我很高兴能与您聊天。您有什么特别想讨论的话题吗？或者您有什么问题想问我？无论是日常生活、工作学习，还是科技、文化、历史等方面的话题，我都很乐意与您交流。请随意开始我们的对话吧！\\n\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Human: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    chat_history += f\"Human: {user_input}\\nAssistant: \"\n",
    "    response = generate_response(chat_history)\n",
    "    chat_history += f\"{response}\\nHuman: \"\n",
    "    \n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "print(\"对话结束!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
